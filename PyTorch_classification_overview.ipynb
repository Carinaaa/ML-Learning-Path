{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz0JRckTSs6da/O5QdVV8H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carinaaa/ML-Learning-Path/blob/main/PyTorch_classification_overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch** is an open-source machine learning library developed by **Facebook's AI Research lab (FAIR)**. It is widely used for deep learning and artificial intelligence applications, offering strong support for building and training neural networks.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "*   An optimized tensor library for deep learning using GPUs and CPUs.\n",
        "*   Dynamic computation graphs, which makes it more flexible and intuitive than static graph frameworks.\n",
        "* Modules for deep learning, including layers, loss functions, and optimizers.\n",
        "\n",
        "* TorchScript for model serialization and production deployment.\n",
        "\n",
        "* Integration with Python, making it easy to debug and prototype.\n",
        "\n",
        "**Typical Use Cases:**\n",
        "\n",
        "* Computer vision (e.g., image classification, segmentation)\n",
        "* Natural language processing (NLP)\n",
        "* Reinforcement learning\n",
        "* Scientific computing"
      ],
      "metadata": {
        "id": "R_tsRviiSRjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pTSgm-wRLc-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import timm\n",
        "\n",
        "import matplotlib.pyplot as plt # For data viz\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print('System Version:', sys.version)\n",
        "print('PyTorch version', torch.__version__)\n",
        "print('Torchvision version', torchvision.__version__)\n",
        "print('Numpy version', np.__version__)\n",
        "print('Pandas version', pd.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After importing the modules, let's setup the datasets.\n",
        "\n",
        "**What are Datasets?**\n",
        "\n",
        "In machine learning, a dataset is a structured collection of data used for training, validating, and testing models.\n",
        "\n",
        "A dataset typically includes:\n",
        "\n",
        "* Input features (e.g., images, text, or tabular data)\n",
        "* Labels or targets (e.g., categories, values, or sequences)\n",
        "\n",
        "Datasets are critical because the performance of any machine learning model heavily depends on the quality and diversity of the data it's trained on."
      ],
      "metadata": {
        "id": "KJQGIXlTWGmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ahemateja19bec1025/traffic-sign-dataset-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "id": "Z-mFs-ihaeMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copytree('/kaggle', \"dataset/kaggle\", dirs_exist_ok=True)"
      ],
      "metadata": {
        "id": "Q3Lm74vz_5d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrafficSignDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data = ImageFolder(data_dir, transform=transform)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    @property\n",
        "    def classes(self):\n",
        "        return self.data.classes"
      ],
      "metadata": {
        "id": "seBk5MnLWFk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TrafficSignDataset(data_dir='/content/dataset/kaggle/input/traffic-sign-dataset-classification/traffic_Data/DATA')"
      ],
      "metadata": {
        "id": "UoBM8Ty-Yvze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "id": "jAyOadxMa-79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[1]"
      ],
      "metadata": {
        "id": "usxco8vocJUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_dataset[1]\n",
        "print(f\"Label: {label}\")\n",
        "image"
      ],
      "metadata": {
        "id": "9Mn5y1GVa_XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_dataset[299]\n",
        "print(f\"Label: {label}\")\n",
        "image"
      ],
      "metadata": {
        "id": "qWu8m--fLaTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a dictionary associating target values with traffic sign names\n"
      ],
      "metadata": {
        "id": "0yhOtAnHcaA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset should be consistent, so we can use transforms. For example, we can resize the image. We will also transform to a tensor.\n",
        "Using those transformations we will recreate the dataset."
      ],
      "metadata": {
        "id": "NL4YEanGOQtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "data_dir = '/content/dataset/kaggle/input/traffic-sign-dataset-classification/traffic_Data/DATA'\n",
        "train_dataset = TrafficSignDataset(data_dir, transform)"
      ],
      "metadata": {
        "id": "es3g7Op2rGMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_dataset[100]\n",
        "image.shape"
      ],
      "metadata": {
        "id": "Ey2J0pGXN3MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Larger batch size = faster per-epoch training, but:\n",
        "\n",
        "* Sometimes slightly worse generalization.\n",
        "\n",
        "* You may want to tune learning rate proportionally"
      ],
      "metadata": {
        "id": "mqsLKNPhjPj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "szQQRebyVy_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in dataloader:\n",
        "  break"
      ],
      "metadata": {
        "id": "r9f_BUkiozUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape, labels.shape"
      ],
      "metadata": {
        "id": "cbMgezwXo5U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "2gy2ddZdJMot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Typical Flow (Full Pipeline)**\n",
        "\n",
        "1.   Dataset: load images + YOLO labels ([class, x, y, w, h])\n",
        "2.   DataLoader: create batches with shuffle=True\n",
        "3.   Model: define or load a model (YOLO, Faster R-CNN, custom CNN)\n",
        "4.   Loss Function: compute error between prediction and ground truth\n",
        "5.   Optimizer: update weights using .backward() and .step()\n",
        "6.   Train Loop: repeat over epochs, improve predictions"
      ],
      "metadata": {
        "id": "-uz6lm-ooSEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTrafficSignClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=58):\n",
        "        super(SimpleTrafficSignClassifier, self).__init__()\n",
        "        # Where we define all the parts of the model\n",
        "        self.base_model = timm.create_model('efficientnet_b0', pretrained=True)\n",
        "        self.features = nn.Sequential(*list(self.base_model.children())[:-1])\n",
        "\n",
        "        enet_out_size = 1280\n",
        "        # Make a classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(enet_out_size, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Connect these parts and return the output\n",
        "        x = self.features(x)\n",
        "        output = self.classifier(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "knBcd4VOtKvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At model definition step, you can either create your own model, defining different kinds of layers or you can import existing models such as fasterrcnn_resnet50_fpn etc.\n",
        "\n",
        "**Example of simple model:**\n",
        "\n",
        "    class SimpleCNN(nn.Module):\n",
        "      def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc = nn.Linear(16 * 111 * 111, num_classes)  # example size\n",
        "\n",
        "      def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "FW7g4k4Pmtn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleTrafficSignClassifier(num_classes=58)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "Epu32JdFlRWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_out = model(images)\n",
        "example_out.shape # [batch_size, num_classes]"
      ],
      "metadata": {
        "id": "8oNMV22on-Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The training loop**"
      ],
      "metadata": {
        "id": "E6q94cdhxMiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function - This compares the model’s output with the ground truth and returns a loss value for backpropagation.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Optimizer - This updates the model's weights using gradients computed during backpropagation.\n",
        "optimezer = optim.Adam(model.parameters(), lr=0.001) # learning rate is constant in this case"
      ],
      "metadata": {
        "id": "AB7VqmwOxsUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup Datasets**\n",
        "\n",
        "Note: the test images need to be in their label folder in order to be used with defined classifier."
      ],
      "metadata": {
        "id": "_20cjwBJLv7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "directory = '/content/dataset/kaggle/input/traffic-sign-dataset-classification/traffic_Data/TEST'\n",
        "\n",
        "for entry in os.scandir(directory):\n",
        "  label_name = str(entry.name[1:3])\n",
        "  print(f\"{label_name} is processing...\")\n",
        "  if label_name[0] == '0': # 00, 01, 02, 03...\n",
        "    label_name = label_name[1]\n",
        "  try:\n",
        "    os.mkdir(\"/content/dataset/kaggle/TEST/\" + label_name)\n",
        "  except FileExistsError:\n",
        "    print(\"Exist, only moving files\")\n",
        "  try:\n",
        "    shutil.move(directory + '/' + entry.name, \"/content/dataset/kaggle/TEST/\" + label_name)\n",
        "  except FileNotFoundError:\n",
        "    print(f\"{entry.name} cannot be copied...\")"
      ],
      "metadata": {
        "id": "v3RdhgmUh6_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(59):\n",
        "  try:\n",
        "    os.mkdir(\"/content/dataset/kaggle/VALID/\" + str(i))\n",
        "  except FileExistsError:\n",
        "    print(\"Folder already created\")\n"
      ],
      "metadata": {
        "id": "sA96sCoxZ37S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_folder = '/content/dataset/kaggle/input/traffic-sign-dataset-classification/traffic_Data/DATA'\n",
        "valid_folder = '/content/dataset/kaggle/TEST' # Should be VALID the folder, but I did not add all the data in there...\n",
        "test_folder = '/content/dataset/kaggle/TEST'\n",
        "\n",
        "train_dataset = TrafficSignDataset(train_folder, transform=transform)\n",
        "val_dataset = TrafficSignDataset(valid_folder, transform=transform)\n",
        "test_dataset = TrafficSignDataset(test_folder, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "JCahUi7rfttY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training loop**\n",
        "\n",
        "A **classification training loop** is the iterative process by which a model learns to categorize input data into predefined classes. It involves feeding data through the model, computing errors, and updating model weights to reduce those errors.\n",
        "\n",
        "Model: efficientnet_b0\n",
        "\n",
        "Loss function: commonly cross-entropy loss for classification.\n",
        "\n",
        "Optimizer: e.g. SGD or Adam, we used Adam\n",
        "\n",
        "An **epoch** is one full pass through the training dataset.\n",
        "\n",
        "The data is divided into **batches** for efficiency.\n",
        "\n",
        "For each batch:\n",
        "\n",
        "* Forward Pass: Pass input data through the model to get predictions.\n",
        "\n",
        "* Loss Calculation: Compare predictions to true labels using the loss function.\n",
        "\n",
        "* Backward Pass: Compute gradients using backpropagation.\n",
        "\n",
        "* Optimizer Step: Update model weights using the optimizer.\n",
        "\n",
        "* (Optional) Track accuracy or other metrics.\n",
        "\n",
        "After each epoch, evaluate the model on a validation set to monitor generalization."
      ],
      "metadata": {
        "id": "12FcglF1VVw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple training loop\n",
        "num_epochs = 5\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SimpleTrafficSignClassifier(num_classes=61)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc='Training loop'):\n",
        "        # Move inputs and labels to the device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward() # using weights\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc='Validation loop'):\n",
        "            # Move inputs and labels to the device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "    val_loss = running_loss / len(val_loader.dataset)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")"
      ],
      "metadata": {
        "id": "XyY7i0BDL21B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(val_losses, label='Validation loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss over epochs\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4i7ikVzpUZK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Training loss:\n",
        "\n",
        "*   is the error calculated on the same data the model is being trained on.\n",
        "*   used to optimize the model's calculated parameters during the training process, aiming to minimize the value\n",
        "*   decreseaing values indicate  that the model is learning effectively\n",
        "\n",
        "\n",
        "Validation loss:\n",
        "\n",
        "*   is the error calculated on a separated dataset, that the model has not seen during the training\n",
        "*   decreasing values indicate that the model is generalizing well on unseen data\n",
        "*   increasing value or plateauing while the training loss decreases, may indicate overfitting\n",
        "\n",
        "Intrepretation: overfitting, probably due to using validation data from the TEST data (?)\n",
        "\n"
      ],
      "metadata": {
        "id": "wZTJ1K4r1KOu"
      }
    }
  ]
}